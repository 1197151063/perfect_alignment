{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn,Tensor,LongTensor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpyg\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgcn_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gcn_norm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn,Tensor,LongTensor\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "import numpy as np\n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.utils import degree\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_sparse import SparseTensor,matmul\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter settings(参数设置)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cprint(words: str):\n",
    "    print(f\"\\033[0;30;43m{words}\\033[0m\")\n",
    "\n",
    "def bprint(words:str):\n",
    "    print(f\"\\033[0;30;45m{words}\\033[0m\")\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU else \"cpu\")\n",
    "embedding_size = 64\n",
    "batch_size = 4096\n",
    "test_batch_size = 1024\n",
    "lr = 1e-3\n",
    "reg_weight = 1e-4\n",
    "layer = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing(加载数据)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    \"\"\"\n",
    "    Loading data from datasets\n",
    "    already supportted:['gowalla','amazon-book','yelp2018','lastfm']\n",
    "    \"\"\"\n",
    "    def __init__(self,path='./data/'):\n",
    "        dir_path = path + 'gowalla'\n",
    "        cprint(f'loading from {dir_path}')\n",
    "        train_file = dir_path + '/train.txt'\n",
    "        test_file = dir_path + '/test.txt'\n",
    "        train_users,train_items = [],[]\n",
    "        test_users,test_items = [],[]\n",
    "        train_edge_index,test_edge_index = [],[]\n",
    "        with open(train_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    all = l.strip('\\n').split(' ')\n",
    "                    uid = int(all[0])\n",
    "                    val = int(len(all) * 0.8)\n",
    "                    items = [int(i) for i in all[1:val]]\n",
    "                    for item in items:\n",
    "                        train_edge_index.append([uid,item])\n",
    "                    train_users.extend([uid] * len(items))\n",
    "                    train_items.extend(items)\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            for l in f.readlines():\n",
    "                if len(l) > 0:\n",
    "                    all = l.strip('\\n').split(' ')\n",
    "                    uid = int(all[0])\n",
    "                    try:\n",
    "                        items = [int(i) for i in all[1:]]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    for item in items:\n",
    "                        test_edge_index.append([uid,item])\n",
    "                    test_users.extend([uid] * len(items))\n",
    "                    test_items.extend(items)\n",
    "        \n",
    "\n",
    "\n",
    "        train_edge_index = torch.LongTensor(np.array(train_edge_index).T)\n",
    "        test_edge_index = torch.LongTensor(np.array(test_edge_index).T)\n",
    "        edge_index = torch.cat((train_edge_index,test_edge_index),1)\n",
    "        self.edge_index = edge_index\n",
    "        num_users = len(torch.unique(edge_index[0]))\n",
    "        num_items = len(torch.unique(edge_index[1]))\n",
    "        mask = torch.zeros(num_users,num_items)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.train_edge_index = train_edge_index\n",
    "        self.test_edge_index = test_edge_index\n",
    "        self.UserItemNet = csr_matrix((np.ones(len(self.edge_index[0])), (self.edge_index[0].numpy(), self.edge_index[1].numpy())),\n",
    "                                      shape=(self.num_users, self.num_items))\n",
    "        # self.test_edge_index = test_edge_index\n",
    "        self.bipartite_graph = self.getSparseBipartite()\n",
    "        self.adj_mat = self.getSparseGraph()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            range(self.train_edge_index.size(1)),\n",
    "            shuffle=True,\n",
    "            batch_size=4096\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            list(range(num_users)),batch_size=1024,shuffle=False,num_workers=5\n",
    "        )\n",
    "        test_ground_truth_list = [[] for _ in range(num_users)]\n",
    "        for i in range(len(test_items)):\n",
    "            test_ground_truth_list[test_users[i]].append(test_items[i])\n",
    "        for i in range(len(train_items)):\n",
    "            mask[train_users[i]][train_items[i]] = -np.inf\n",
    "        self.test_ground_truth_list = test_ground_truth_list\n",
    "        self.mask = mask\n",
    "    '''\n",
    "    A = |0   R|\n",
    "        |R^T 0|\n",
    "    R : user-item bipartite graph\n",
    "    A : unnormalized Adjacency Matrix\n",
    "    '''\n",
    "    def getSparseGraph(self):\n",
    "        cprint(\"generate Adjacency Matrix A\")\n",
    "        user_index = self.train_edge_index[0]\n",
    "        item_index = self.train_edge_index[1]\n",
    "        row_index = torch.cat([user_index,item_index+self.num_users])\n",
    "        col_index = torch.cat([item_index+self.num_users,user_index])\n",
    "        return SparseTensor(row=row_index,col=col_index,sparse_sizes=(self.num_items+self.num_users,self.num_items+self.num_users))\n",
    "\n",
    "    def getSparseBipartite(self):\n",
    "        user_index = self.train_edge_index[0]\n",
    "        item_index = self.train_edge_index[1]\n",
    "        return SparseTensor(row=user_index,col=item_index,sparse_sizes=(self.num_users,self.num_items))\n",
    "    \n",
    "    def get_user_all_interacted(self,users):\n",
    "        users = users.detach().cpu().numpy()\n",
    "        posItems = []\n",
    "        for user in users:\n",
    "            posItems.append(self.UserItemNet[user].nonzero()[1])\n",
    "        return posItems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph Mode(定义图模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecModel(MessagePassing):\n",
    "    def __init__(self,\n",
    "                 num_users:int,\n",
    "                 num_items:int,\n",
    "                 edge_index:LongTensor):\n",
    "        super().__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_nodes = num_users + num_items\n",
    "        self.f = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def get_sparse_graph(self,\n",
    "                         edge_index,\n",
    "                         use_value=False,\n",
    "                         value=None):\n",
    "        num_users = self.num_users\n",
    "        num_nodes = self.num_nodes\n",
    "        r,c = edge_index\n",
    "        row = torch.cat([r , c + num_users])\n",
    "        col = torch.cat([c + num_users , r])\n",
    "        if use_value:\n",
    "            value = torch.cat([value,value])\n",
    "            return SparseTensor(row=row,col=col,value=value,sparse_sizes=(num_nodes,num_nodes))\n",
    "        else:\n",
    "            return SparseTensor(row=row,col=col,sparse_sizes=(num_nodes,num_nodes))\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self,\n",
    "                edge_label_index:Tensor):\n",
    "        out = self.get_embedding()\n",
    "        out_u,out_i = torch.split(out,[self.num_users,self.num_items])\n",
    "        out_src = out_u[edge_label_index[0]]\n",
    "        out_dst = out_i[edge_label_index[1]]\n",
    "        out_dst_neg = out_i[edge_label_index[2]]\n",
    "        return (out_src * out_dst).sum(dim=-1),(out_src * out_dst_neg).sum(dim=-1)\n",
    "    \n",
    "    def link_prediction(self,\n",
    "                        src_index:Tensor=None,\n",
    "                        dst_index:Tensor=None):\n",
    "        out = self.get_embedding()\n",
    "        out_u,out_i = torch.split(out,[self.num_users,self.num_items])\n",
    "        if src_index is None:\n",
    "            src_index = torch.arange(self.num_users).long()\n",
    "        if dst_index is None:\n",
    "            dst_index = torch.arange(self.num_items).long()\n",
    "        out_src = out_u[src_index]\n",
    "        out_dst = out_i[dst_index]\n",
    "        pred = out_src @ out_dst.t()\n",
    "        return pred.sigmoid()\n",
    "    \n",
    "    def recommendation_loss(self,\n",
    "                            pos_rank,\n",
    "                            neg_rank,\n",
    "                            edge_label_index):\n",
    "        rec_loss = torch.nn.functional.softplus(neg_rank - pos_rank).mean()\n",
    "        user_emb = self.user_emb.weight\n",
    "        item_emb = self.item_emb.weight\n",
    "        embedding = torch.cat([user_emb[edge_label_index[0]],\n",
    "                               item_emb[edge_label_index[1]],\n",
    "                               item_emb[edge_label_index[2]]])\n",
    "        regularization = reg_weight * (1/2) * embedding.norm(p=2).pow(2)\n",
    "        regularization = regularization / pos_rank.size(0)\n",
    "        return rec_loss , regularization\n",
    "    \n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "    \n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(RecModel):\n",
    "    def __init__(self,\n",
    "                 num_users:int,\n",
    "                 num_items:int,\n",
    "                 edge_index:LongTensor):\n",
    "        super().__init__(\n",
    "            num_users=num_users,\n",
    "            num_items=num_items,\n",
    "            edge_index=edge_index\n",
    "        )\n",
    "        self.user_emb = nn.Embedding(num_embeddings=num_users,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        self.item_emb = nn.Embedding(num_embeddings=num_items,\n",
    "                                     embedding_dim=embedding_size)\n",
    "        nn.init.normal_(self.user_emb.weight,std=0.1)\n",
    "        nn.init.normal_(self.item_emb.weight,std=0.1)\n",
    "        self.K = 3\n",
    "        edge_index = self.get_sparse_graph(edge_index=edge_index,use_value=False,value=None)\n",
    "        self.edge_index = gcn_norm(edge_index)\n",
    "        self.alpha= 1./ (1 + self.K)\n",
    "        if isinstance(self.alpha, Tensor):\n",
    "            assert self.alpha.size(0) == self.K + 1\n",
    "        else:\n",
    "            self.alpha = torch.tensor([self.alpha] * (self.K + 1))\n",
    "        print('Go LightGCN')\n",
    "        print(f\"params settings: \\n emb_size:{embedding_size}\\n L2 reg:{reg_weight}\\n layer:{self.K}\")\n",
    "\n",
    "    def get_embedding(self):\n",
    "        x_u=self.user_emb.weight\n",
    "        x_i=self.item_emb.weight\n",
    "        x=torch.cat([x_u,x_i])\n",
    "        out = x * self.alpha[0]\n",
    "        for i in range(self.K):\n",
    "            x = self.propagate(edge_index=self.edge_index,x=x)\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "        return out\n",
    "\n",
    "    def instance_loss(self,edge_label_index):\n",
    "        out = self.get_embedding()\n",
    "        users,items = torch.split(out,[self.num_users,self.num_items])\n",
    "        user_emb = users[edge_label_index[0]]\n",
    "        item_pos = items[edge_label_index[1]]\n",
    "        item_neg = items[edge_label_index[2]]\n",
    "        return ((user_emb * item_pos).sum(dim=-1) - (user_emb * item_neg).sum(dim=-1)).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics(模型评估指标)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(k_values:list,\n",
    "         model,\n",
    "         train_edge_index,\n",
    "         test_edge_index,\n",
    "         num_users,\n",
    "         ):\n",
    "    model.eval()\n",
    "    recall = {k: 0 for k in k_values}\n",
    "    ndcg = {k: 0 for k in k_values}\n",
    "    total_examples = 0\n",
    "    for start in range(0, num_users, 1024):\n",
    "        end = start + 1024\n",
    "        if end > num_users:\n",
    "            end = num_users\n",
    "        src_index=torch.arange(start,end).long().to(device)\n",
    "        logits = model.link_prediction(src_index=src_index,dst_index=None)\n",
    "\n",
    "        # Exclude training edges:\n",
    "        mask = ((train_edge_index[0] >= start) &\n",
    "                (train_edge_index[0] < end))\n",
    "        masked_interactions = train_edge_index[:,mask]\n",
    "        logits[masked_interactions[0] - start,masked_interactions[1]] = float('-inf')\n",
    "        # Generate ground truth matrix\n",
    "        ground_truth = torch.zeros_like(logits, dtype=torch.bool)\n",
    "        mask = ((test_edge_index[0] >= start) &\n",
    "                (test_edge_index[0] < end))\n",
    "        masked_interactions = test_edge_index[:,mask]\n",
    "        ground_truth[masked_interactions[0] - start,masked_interactions[1]] = True\n",
    "        node_count = degree(test_edge_index[0, mask] - start,\n",
    "                            num_nodes=logits.size(0))\n",
    "        topk_indices = logits.topk(max(k_values),dim=-1).indices\n",
    "        for k in k_values:\n",
    "            topk_index = topk_indices[:,:k]\n",
    "            isin_mat = ground_truth.gather(1, topk_index)\n",
    "            # Calculate recall\n",
    "            recall[k] += float((isin_mat.sum(dim=-1) / node_count.clamp(1e-6)).sum())\n",
    "            # Calculate NDCG\n",
    "            log_positions = torch.log2(torch.arange(2, k + 2, device=logits.device).float())\n",
    "            dcg = (isin_mat / log_positions).sum(dim=-1)\n",
    "            ideal_dcg = torch.zeros_like(dcg)\n",
    "            for i in range(len(dcg)):\n",
    "                ideal_dcg[i] = (1.0 / log_positions[:node_count[i].clamp(0, k).int()]).sum()\n",
    "            ndcg[k] += float((dcg / ideal_dcg.clamp(min=1e-6)).sum())\n",
    "\n",
    "        total_examples += int((node_count > 0).sum())\n",
    "\n",
    "    recall = {k: recall[k] / total_examples for k in k_values}\n",
    "    ndcg = {k: ndcg[k] / total_examples for k in k_values}\n",
    "\n",
    "    return recall,ndcg\n",
    "\n",
    "\n",
    "def Fast_Sampling(dataset):\n",
    "    \"\"\"\n",
    "    A more efficient sampler with simplified negative sampling\n",
    "    easy to overfit on raw GNN model\n",
    "    \"\"\"\n",
    "    train_edge_index = dataset.train_edge_index.to(device)\n",
    "    num_items = dataset.num_items\n",
    "    mini_batch = []\n",
    "    train_loader = DataLoader(\n",
    "            range(train_edge_index.size(1)),\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size)\n",
    "    for index in train_loader:\n",
    "        pos_edge_label_index = train_edge_index[:,index]\n",
    "        neg_edge_label_index = torch.randint(0, num_items,(index.numel(), ), device=device)\n",
    "        edge_label_index = torch.stack([\n",
    "            pos_edge_label_index[0],\n",
    "            pos_edge_label_index[1],\n",
    "            neg_edge_label_index,\n",
    "        ])\n",
    "        mini_batch.append(edge_label_index)\n",
    "    return mini_batch    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Training(采样&训练)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpr(dataset,\n",
    "                  model:LightGCN,\n",
    "                  opt):\n",
    "    model = model\n",
    "    model.train()\n",
    "    S = Fast_Sampling(dataset=dataset)\n",
    "    aver_loss = 0.\n",
    "    total_batch = len(S)\n",
    "    for edge_label_index in S:\n",
    "        pos_rank,neg_rank = model(edge_label_index)\n",
    "        bpr_loss,L2_reg = model.recommendation_loss(pos_rank,neg_rank,edge_label_index)\n",
    "        loss = bpr_loss + L2_reg\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()    \n",
    "        aver_loss += (bpr_loss + L2_reg)\n",
    "    aver_loss /= total_batch\n",
    "    return f\"average loss {aver_loss:5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Loader()\n",
    "train_edge_index = dataset.train_edge_index.to(device)\n",
    "test_edge_index = dataset.test_edge_index.to(device)\n",
    "num_users = dataset.num_users\n",
    "num_items = dataset.num_items\n",
    "model = LightGCN(num_users=num_users,\n",
    "                 num_items=num_items,\n",
    "                 edge_index=train_edge_index).to(device)\n",
    "opt = torch.optim.Adam(params=model.parameters(),lr=lr)\n",
    "best = 0.\n",
    "patience = 0.\n",
    "max_score = 0.\n",
    "for epoch in range(1, 1001):\n",
    "    loss = train_bpr(dataset=dataset,model=model,opt=opt)\n",
    "    recall,ndcg = test([20,50],model,train_edge_index,test_edge_index,num_users)\n",
    "    print(f'Epoch: {epoch:03d}, {loss}, R@20: '\n",
    "          f'{recall[20]:.4f}, R@50: {recall[50]:.4f} '\n",
    "          f', N@20: {ndcg[20]:.4f}, N@50: {ndcg[50]:.4f}')\n",
    "    if epoch % 5 == 0:\n",
    "        print(model.link_prediction(torch.arange(5).to(device),torch.arange(5).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
